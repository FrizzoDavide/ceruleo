{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rul_pm.dataset.CMAPSS import CMAPSSDataset, sensor_indices\n",
    "from rul_pm.transformation.transformers import Transformer, transformation_pipeline\n",
    "from rul_pm.models.keras import  KerasTrainableModel, XiangQiangJianQiaoModel\n",
    "from rul_pm.models.keras.losses import weighted_categorical_crossentropy\n",
    "from rul_pm.models.keras.attention import Attention\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from rul_pm.iterators.batcher import get_batcher, dataset_map, get_features\n",
    "from tensorflow.keras import backend as K, Model, Input, optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TimeDistributed \n",
    "from tensorflow.keras.layers import (Layer, Conv1D, Dense, Add, Input, AveragePooling1D,\n",
    "                                     BatchNormalization, LayerNormalization, Flatten,\n",
    "                                     Concatenate, Bidirectional, Conv2D, MaxPooling1D,\n",
    "                                     Permute, TimeDistributed, Multiply, LeakyReLU,    \n",
    "                                     GlobalAveragePooling1D, MaxPooling2D,\n",
    "                                     Softmax, AveragePooling2D,\n",
    "                                     ConvLSTM2D ,SpatialDropout2D, LocallyConnected1D,\n",
    "                                     multiply, concatenate, Activation, Masking,\n",
    "                                     Masking, LSTM, GRU, MaxPool1D, Conv1D, Dropout, Average,\n",
    "                                     Reshape, UpSampling1D, AveragePooling1D,GaussianNoise,\n",
    "                                     Concatenate, Bidirectional)\n",
    "from tcn import TCN\n",
    "from tensorflow.keras import backend as K, Model, Input, optimizers\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FD001\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CMAPSSDataset(train=True, model='FD001')\n",
    "test_dataset = CMAPSSDataset(train=False, model='FD001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors = ['SensorMeasure1',\n",
    "       'SensorMeasure2', 'SensorMeasure3', 'SensorMeasure4', 'SensorMeasure5',\n",
    "       'SensorMeasure6', 'SensorMeasure7', 'SensorMeasure8', 'SensorMeasure9',\n",
    "       'SensorMeasure10', 'SensorMeasure11', 'SensorMeasure12',\n",
    "       'SensorMeasure13', 'SensorMeasure14', 'SensorMeasure15',\n",
    "       'SensorMeasure16', 'SensorMeasure17', 'SensorMeasure18',\n",
    "       'SensorMeasure19', 'SensorMeasure20', 'SensorMeasure21']\n",
    "\n",
    "s1 = [sensors[i-1] for i in sensor_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawAndBinClasses(BaseEstimator, TransformerMixin):   \n",
    "    \"\"\"\n",
    "        A target transformer that outputs\n",
    "        the RUL + nbins binary vectors\n",
    "    \"\"\"\n",
    "    def __init__(self, nbins):\n",
    "        self.nbins = nbins \n",
    "        \n",
    "        \n",
    "    def fit(self, X, y=None):       \n",
    "        self.max_RUL = int(X.max())\n",
    "        self.value_ranges = np.linspace(0, self.max_RUL, num=self.nbins+1)        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        v = X        \n",
    "        classes = []\n",
    "        for j in range(len(self.value_ranges)-1):                 \n",
    "            lower = self.value_ranges[j] \n",
    "            upper = self.value_ranges[j+1] \n",
    "            classes.append(((v>=lower) & (v <upper)))        \n",
    "        v = np.vstack((v, *classes)).T\n",
    "        return v\n",
    "\n",
    "class SoftmaxRegression(KerasTrainableModel):\n",
    "    \"\"\"\n",
    "    The network contains stacked layers of 1-dimensional convolutional layers\n",
    "    followed by max poolings\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    self: list of tuples (filters: int, kernel_size: int)\n",
    "          Each element of the list is a layer of the network. The first element of the tuple contaings\n",
    "          the number of filters, the second one, the kernel size.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, raw_and_bins, alpha, window, batch_size, step, transformer, shuffle, models_path,\n",
    "                 patience=4, cache_size=30, output_size=3, padding='same'):\n",
    "        super(SoftmaxRegression, self).__init__(window,\n",
    "                                                  batch_size,\n",
    "                                                  step,\n",
    "                                                  transformer,\n",
    "                                                  shuffle,\n",
    "                                                  models_path,\n",
    "                                                  patience=patience,\n",
    "                                                  output_size=output_size,\n",
    "                                                  cache_size=30,\n",
    "                                                  callbacks=[])\n",
    "        if raw_and_bins is not None:\n",
    "            self.raw_and_bins = raw_and_bins\n",
    "            weights = [1 for _ in range(self.raw_and_bins.nbins)]\n",
    "            self.wcc = weighted_categorical_crossentropy(weights)\n",
    "            self.output_size = self.raw_and_bins.nbins\n",
    "        else:\n",
    "            self.output_size = 1\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    \n",
    "    def _generate_batcher(self, train_batcher, val_batcher):\n",
    "        n_features = self.transformer.n_features\n",
    "        def gen_train():\n",
    "            for X, y in train_batcher:\n",
    "                yield X, y\n",
    "\n",
    "        def gen_val():\n",
    "            for X, y in val_batcher:\n",
    "                yield X, y\n",
    "\n",
    "        a = tf.data.Dataset.from_generator(\n",
    "            gen_train, (tf.float32, tf.float32), \n",
    "            (tf.TensorShape([None, self.window, n_features]), \n",
    "             tf.TensorShape([None, self.output_size])))\n",
    "        b = tf.data.Dataset.from_generator(\n",
    "            gen_val, (tf.float32, tf.float32), \n",
    "            (tf.TensorShape([None, self.window, n_features]), \n",
    "             tf.TensorShape([None, self.output_size])))\n",
    "        return a,b\n",
    "    \n",
    "    def _loss(self, y_true, y_pred):\n",
    "        # cross entropy loss\n",
    "        bin_true = y_true[:,1:]\n",
    "        cont_true = y_true[:,0]        \n",
    "        \n",
    "        #y_pred_rul = y_pred[:, 0]\n",
    "        #y_pred_bins = y_pred[:, 1:]\n",
    "        y_pred_bins = y_pred\n",
    "        cls_loss = self.wcc(bin_true, y_pred_bins)\n",
    "        # MSE loss\n",
    "        idx_tensor = self.raw_and_bins.value_ranges[:-1]\n",
    "        pred_cont = tf.reduce_sum(y_pred_bins * idx_tensor, 1)\n",
    "        #pred_cont = tf.keras.backend.argmax(y_pred, axis=1)\n",
    "        rmse_loss_softmax = tf.losses.mean_squared_error(cont_true, pred_cont)\n",
    "        \n",
    "        #mse_loss = tf.losses.mean_squared_error(cont_true, y_pred_rul)\n",
    "        # Total loss\n",
    "        total_loss = (cls_loss +\n",
    "                      self.alpha * rmse_loss_softmax\n",
    "                     )\n",
    "        return total_loss\n",
    "    \n",
    "\n",
    "    def mse_softmax(self, y_true, y_pred):\n",
    "         # cross entropy loss\n",
    "        bin_true = y_true[:,1:]\n",
    "        cont_true = y_true[:,0]        \n",
    "        \n",
    "        # y_pred_rul = y_pred[:, 0]\n",
    "        # y_pred_bins = y_pred[:, 1:]\n",
    "        y_pred_bins = y_pred\n",
    "        idx_tensor = self.raw_and_bins.value_ranges[:-1]\n",
    "        pred_cont = tf.reduce_sum(y_pred_bins * idx_tensor, 1)\n",
    "        #pred_cont = tf.keras.backend.argmax(y_pred, axis=1)\n",
    "        return tf.sqrt(tf.losses.mean_squared_error(cont_true, pred_cont))\n",
    "    \n",
    "\n",
    "    def mse_rul(self,  y_true, y_pred):\n",
    "         # cross entropy loss\n",
    "        cont_true = y_true[:,0]        \n",
    "        y_pred_rul = y_pred[:, 0]\n",
    "        return tf.losses.mean_squared_error(cont_true, y_pred_rul)     \n",
    "        \n",
    "    def compile(self):\n",
    "        self.compiled=  True\n",
    "        self.model.compile(loss='mse', \n",
    "\n",
    "                           optimizer=tf.keras.optimizers.Adam(lr=0.0001))\n",
    "        \n",
    "\n",
    "\n",
    "    def build_model(self):\n",
    "        from collections import namedtuple\n",
    "        #function to split the input in multiple outputs\n",
    "        def splitter(x):\n",
    "            return [x[:,:,i:i+1] for i in range(n_features)]\n",
    "\n",
    "        n_features = self.transformer.n_features\n",
    "        \n",
    "        i = Input(shape=(self.window, n_features))   \n",
    "        \n",
    "\n",
    "\n",
    "        m = TCN(use_skip_connections=True, \n",
    "                use_batch_norm=True, \n",
    "                return_sequences=True,\n",
    "                dropout_rate=0.1)(i)    \n",
    "        m = Attention(64, self.window-1)(m)\n",
    "        m = Dense(100, activation='relu')(m)                \n",
    "        m = Dropout(0.5)(m)        \n",
    "        m = BatchNormalization()(m)\n",
    "        proba = Dense(150, activation='relu')(m)        \n",
    "        proba = BatchNormalization()(proba)\n",
    "        proba = Dropout(0.1)(proba)\n",
    "        proba = Dense(1, activation='linear')(proba)\n",
    "        \n",
    "\n",
    "        return Model(inputs = i, outputs =proba)\n",
    "\n",
    "\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return 'ConvolutionalSimple'\n",
    "\n",
    "    def get_params(self, deep=False):\n",
    "        params = super().get_params()\n",
    "        params.update({\n",
    "        })\n",
    "        return params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Remaining useful life estimation in prognostics using deep convolution neural networks\n",
    "Author \n",
    "* Xiang Li\n",
    "* Qian Ding\n",
    "* Jian-Qiao Sunc\n",
    "\n",
    "https://www.sciencedirect.com/science/article/pii/S0951832017307779?casa_token=DHKuBa83HrcAAAAA:-U6kHeTYyqmo9XYB8Wm-hOFOx-2IMOC_o5bhZEpdEW8tTB8zliBx9kzxiFuqX6pu_lf7nAQDqeq-#tbl0002\n",
    "\n",
    "\n",
    "A deep learning method for prognostics is proposed based on convolution neural networks. Dropout technique is employed to relieve overfitting problem. Experiments are carried out on the popular C-MAPSS dataset to show the effectiveness of the proposed method. The goal of the task is to estimate the remaining useful life of aero-engine units accurately. With raw feature selection, data pre-processing and sample preparation using time window, good prognostic performance is achieved with the proposed method, and small error between the prediction and the actual RUL value is obtained for the testing data. The RUL in the life-time of the engine units can be well predicted, especially for the late period close to failure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "                'RUL',\n",
    "                transformation_pipeline(                                \n",
    "                    scaler=MinMaxScaler((-1, 1)),\n",
    "                    features=sensors\n",
    "                )\n",
    "            )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true), axis=0)) \n",
    "    \n",
    "def scheduler(epoch, lr): \n",
    "    if epoch < 200:\n",
    "        return 0.001\n",
    "    else:\n",
    "        return 0.0001\n",
    "    \n",
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "model = XiangQiangJianQiaoModel(\n",
    "            10,\n",
    "            10,           \n",
    "            dropout=0.5,\n",
    "            window=30,\n",
    "            batch_size=32,\n",
    "            step = 1,\n",
    "            transformer=transformer,\n",
    "            shuffle='all',\n",
    "            models_path=Path('.'),\n",
    "            patience=500,\n",
    "            cache_size=40,\n",
    "            callbacks=[lr_callback],\n",
    "            learning_rate=1e-3,\n",
    "            loss=root_mean_squared_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5af51f9e83a4bdb9bf19eab57f06e08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45b0dd106381431d98da9306e1dc9f25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_matrix(df):\n",
    "    w = 30\n",
    "    X = []\n",
    "    y = []\n",
    "    for life in df['life'].unique():\n",
    "        df_life = df[df['life'] == life].reset_index()\n",
    "        for i in range(0, df_life.shape[0]-w):\n",
    "            X.append(np.expand_dims(df_life.iloc[i:i+w, :][s1].values, 0))\n",
    "            y.append(np.expand_dims(df_life.iloc[i+w, :]['RUL'], 0))\n",
    "    return np.concatenate(X), np.concatenate(y)\n",
    "\n",
    "scaler = MinMaxScaler((-1, 1))\n",
    "train_df = train_dataset.toPandas()\n",
    "test_df = test_dataset.toPandas()\n",
    "\n",
    "train_df[sensors] = X_train = scaler.fit_transform(train_df[sensors])\n",
    "\n",
    "test_df[sensors] = X_train = scaler.transform(test_df[sensors])\n",
    "\n",
    "X_train, y_train = get_matrix(train_df)\n",
    "X_test, y_test = get_matrix(test_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rul_pm.models.keras.layers import ExpandDimension\n",
    "from tensorflow.keras.layers import (GRU, LSTM, RNN, Activation, Add,\n",
    "                                     AveragePooling1D, BatchNormalization,\n",
    "                                     Bidirectional, Concatenate, Conv1D,\n",
    "                                     Conv2D, Dense, Dropout, Flatten,\n",
    "                                     GaussianNoise, Lambda, Layer,\n",
    "                                     LayerNormalization, LSTMCell, Masking,\n",
    "                                     MaxPool1D, Permute, Reshape,\n",
    "                                     SpatialDropout1D, StackedRNNCells,\n",
    "                                     UpSampling1D, ZeroPadding2D)\n",
    "import math \n",
    "\n",
    "n_filters = 10\n",
    "filter_size = 10\n",
    "dropout = 0.5\n",
    "n_features = len(s1)\n",
    "input = Input(shape=(30, n_features))\n",
    "x = input\n",
    "\n",
    "x = ExpandDimension()(x)\n",
    "x = Conv2D(n_filters, (filter_size, 1),\n",
    "           padding='same', activation='tanh',\n",
    "           )(x)\n",
    "x = Conv2D(n_filters, (filter_size, 1),\n",
    "           padding='same', activation='tanh',\n",
    "           )(x)\n",
    "x = Conv2D(n_filters, (filter_size, 1),\n",
    "           padding='same', activation='tanh',\n",
    "           )(x)\n",
    "x = Conv2D(n_filters, (filter_size, 1),\n",
    "           padding='same', activation='tanh')(x)\n",
    "x = Conv2D(1, (3,1), padding='same', activation='tanh')(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "x = Dropout(dropout)(x)\n",
    "x = Dense(100,\n",
    "          activation='tanh')(x)\n",
    "output = Dense(1)(x)\n",
    "mm = Model(\n",
    "    inputs=[input],\n",
    "    outputs=[output],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.any(np.isnan(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         [(None, 30, 14)]          0         \n",
      "_________________________________________________________________\n",
      "lambda_8 (Lambda)            (None, 30, 14, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_40 (Conv2D)           (None, 30, 14, 10)        110       \n",
      "_________________________________________________________________\n",
      "conv2d_41 (Conv2D)           (None, 30, 14, 10)        1010      \n",
      "_________________________________________________________________\n",
      "conv2d_42 (Conv2D)           (None, 30, 14, 10)        1010      \n",
      "_________________________________________________________________\n",
      "conv2d_43 (Conv2D)           (None, 30, 14, 10)        1010      \n",
      "_________________________________________________________________\n",
      "conv2d_44 (Conv2D)           (None, 30, 14, 1)         31        \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 420)               0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 420)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 100)               42100     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 45,372\n",
      "Trainable params: 45,372\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "35/35 [==============================] - 5s 151ms/step - loss: 84.1211 - val_loss: 111.9222\n",
      "Epoch 2/200\n",
      "35/35 [==============================] - 5s 147ms/step - loss: 78.8028 - val_loss: 108.4413\n",
      "Epoch 3/200\n",
      "35/35 [==============================] - 5s 147ms/step - loss: 75.8333 - val_loss: 105.0926\n",
      "Epoch 4/200\n",
      "35/35 [==============================] - 5s 147ms/step - loss: 73.0175 - val_loss: 101.8672\n",
      "Epoch 5/200\n",
      "35/35 [==============================] - 5s 147ms/step - loss: 70.3337 - val_loss: 98.7436\n",
      "Epoch 6/200\n",
      "35/35 [==============================] - 5s 147ms/step - loss: 67.7120 - val_loss: 95.7018\n",
      "Epoch 7/200\n",
      "35/35 [==============================] - 5s 149ms/step - loss: 65.1452 - val_loss: 92.7531\n",
      "Epoch 8/200\n",
      "35/35 [==============================] - 5s 147ms/step - loss: 62.6924 - val_loss: 89.8942\n",
      "Epoch 9/200\n",
      "35/35 [==============================] - 5s 148ms/step - loss: 60.4104 - val_loss: 87.1322\n",
      "Epoch 10/200\n",
      "35/35 [==============================] - 5s 148ms/step - loss: 58.2565 - val_loss: 84.4930\n",
      "Epoch 11/200\n",
      "35/35 [==============================] - 5s 147ms/step - loss: 56.2011 - val_loss: 81.8985\n",
      "Epoch 12/200\n",
      "35/35 [==============================] - 5s 155ms/step - loss: 54.2912 - val_loss: 79.4213\n",
      "Epoch 13/200\n",
      "35/35 [==============================] - 6s 164ms/step - loss: 52.4051 - val_loss: 77.0128\n",
      "Epoch 14/200\n",
      "35/35 [==============================] - 6s 157ms/step - loss: 50.6757 - val_loss: 74.7311\n",
      "Epoch 15/200\n",
      "35/35 [==============================] - 5s 155ms/step - loss: 49.0306 - val_loss: 72.5745\n",
      "Epoch 16/200\n",
      "35/35 [==============================] - 6s 161ms/step - loss: 47.4659 - val_loss: 70.4640\n",
      "Epoch 17/200\n",
      "35/35 [==============================] - 6s 160ms/step - loss: 46.0136 - val_loss: 68.4663\n",
      "Epoch 18/200\n",
      "35/35 [==============================] - 5s 150ms/step - loss: 44.5813 - val_loss: 66.4903\n",
      "Epoch 19/200\n",
      "35/35 [==============================] - 5s 149ms/step - loss: 43.2962 - val_loss: 64.6587\n",
      "Epoch 20/200\n",
      "35/35 [==============================] - 5s 150ms/step - loss: 41.9914 - val_loss: 63.1381\n",
      "Epoch 21/200\n",
      "35/35 [==============================] - 5s 148ms/step - loss: 40.8642 - val_loss: 61.2501\n",
      "Epoch 22/200\n",
      "35/35 [==============================] - 5s 147ms/step - loss: 39.7764 - val_loss: 59.5881\n",
      "Epoch 23/200\n",
      "35/35 [==============================] - 5s 148ms/step - loss: 38.6527 - val_loss: 58.3224\n",
      "Epoch 24/200\n",
      "35/35 [==============================] - 5s 147ms/step - loss: 37.6958 - val_loss: 56.5264\n",
      "Epoch 25/200\n",
      "35/35 [==============================] - 5s 148ms/step - loss: 36.7147 - val_loss: 55.3523\n",
      "Epoch 26/200\n",
      "35/35 [==============================] - 5s 148ms/step - loss: 35.8953 - val_loss: 54.4297\n",
      "Epoch 27/200\n",
      "35/35 [==============================] - 5s 148ms/step - loss: 35.1168 - val_loss: 52.5226\n",
      "Epoch 28/200\n",
      "35/35 [==============================] - 5s 147ms/step - loss: 34.3172 - val_loss: 51.2966\n",
      "Epoch 29/200\n",
      "35/35 [==============================] - 5s 147ms/step - loss: 33.6457 - val_loss: 50.2578\n",
      "Epoch 30/200\n",
      "35/35 [==============================] - 5s 148ms/step - loss: 32.9869 - val_loss: 49.4643\n",
      "Epoch 31/200\n",
      "35/35 [==============================] - 5s 148ms/step - loss: 32.6284 - val_loss: 48.1067\n",
      "Epoch 32/200\n",
      "35/35 [==============================] - 5s 148ms/step - loss: 31.8263 - val_loss: 47.2617\n",
      "Epoch 33/200\n",
      "35/35 [==============================] - 5s 149ms/step - loss: 31.2522 - val_loss: 46.2276\n",
      "Epoch 34/200\n",
      "35/35 [==============================] - 5s 148ms/step - loss: 30.8687 - val_loss: 45.7649\n",
      "Epoch 35/200\n",
      "35/35 [==============================] - 5s 147ms/step - loss: 30.3614 - val_loss: 44.5529\n",
      "Epoch 36/200\n",
      "35/35 [==============================] - 5s 148ms/step - loss: 29.7414 - val_loss: 44.0854\n",
      "Epoch 37/200\n",
      "35/35 [==============================] - 5s 148ms/step - loss: 29.6033 - val_loss: 43.1020\n",
      "Epoch 38/200\n",
      "35/35 [==============================] - 5s 154ms/step - loss: 29.0040 - val_loss: 42.3059\n",
      "Epoch 39/200\n",
      "35/35 [==============================] - 5s 148ms/step - loss: 28.6236 - val_loss: 41.9970\n",
      "Epoch 40/200\n",
      "35/35 [==============================] - 5s 149ms/step - loss: 28.4076 - val_loss: 41.2262\n",
      "Epoch 41/200\n",
      "35/35 [==============================] - 5s 149ms/step - loss: 27.9619 - val_loss: 40.4163\n",
      "Epoch 42/200\n",
      "35/35 [==============================] - 5s 148ms/step - loss: 27.6190 - val_loss: 39.9055\n",
      "Epoch 43/200\n",
      "35/35 [==============================] - 5s 149ms/step - loss: 27.5519 - val_loss: 39.4109\n",
      "Epoch 44/200\n",
      "35/35 [==============================] - 5s 148ms/step - loss: 27.0054 - val_loss: 38.7782\n",
      "Epoch 45/200\n",
      "35/35 [==============================] - 5s 149ms/step - loss: 26.7709 - val_loss: 39.2129\n",
      "Epoch 46/200\n",
      "35/35 [==============================] - 5s 148ms/step - loss: 26.5305 - val_loss: 39.3319\n",
      "Epoch 47/200\n",
      "35/35 [==============================] - 5s 151ms/step - loss: 26.3514 - val_loss: 37.3834\n",
      "Epoch 48/200\n",
      "35/35 [==============================] - 5s 149ms/step - loss: 26.1990 - val_loss: 37.3141\n",
      "Epoch 49/200\n",
      "35/35 [==============================] - 5s 149ms/step - loss: 25.8787 - val_loss: 36.5496\n",
      "Epoch 50/200\n",
      "35/35 [==============================] - 5s 148ms/step - loss: 25.6834 - val_loss: 36.4090\n",
      "Epoch 51/200\n",
      "35/35 [==============================] - 5s 148ms/step - loss: 25.4419 - val_loss: 35.9103\n",
      "Epoch 52/200\n",
      "35/35 [==============================] - 5s 149ms/step - loss: 25.1656 - val_loss: 35.4402\n",
      "Epoch 53/200\n",
      "35/35 [==============================] - 5s 148ms/step - loss: 25.1459 - val_loss: 35.0893\n",
      "Epoch 54/200\n",
      "35/35 [==============================] - 5s 149ms/step - loss: 25.0960 - val_loss: 34.9518\n",
      "Epoch 55/200\n",
      "35/35 [==============================] - 5s 148ms/step - loss: 24.6936 - val_loss: 34.4832\n",
      "Epoch 56/200\n",
      "35/35 [==============================] - 5s 148ms/step - loss: 24.5404 - val_loss: 34.1375\n",
      "Epoch 57/200\n",
      "35/35 [==============================] - 5s 149ms/step - loss: 24.2598 - val_loss: 33.9911\n",
      "Epoch 58/200\n",
      "35/35 [==============================] - 5s 149ms/step - loss: 24.2305 - val_loss: 33.8842\n",
      "Epoch 59/200\n",
      "35/35 [==============================] - 5s 149ms/step - loss: 24.0678 - val_loss: 34.4417\n",
      "Epoch 60/200\n",
      "35/35 [==============================] - 5s 148ms/step - loss: 24.0261 - val_loss: 32.9785\n",
      "Epoch 61/200\n",
      "35/35 [==============================] - 5s 149ms/step - loss: 23.6937 - val_loss: 33.5712\n",
      "Epoch 62/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 5s 148ms/step - loss: 23.8077 - val_loss: 32.5094\n",
      "Epoch 63/200\n",
      "35/35 [==============================] - 5s 147ms/step - loss: 23.4604 - val_loss: 33.6729\n",
      "Epoch 64/200\n",
      "35/35 [==============================] - 5s 147ms/step - loss: 23.6203 - val_loss: 33.3799\n",
      "Epoch 65/200\n",
      "35/35 [==============================] - 5s 147ms/step - loss: 23.3703 - val_loss: 31.8527\n",
      "Epoch 66/200\n",
      "35/35 [==============================] - 5s 149ms/step - loss: 23.2162 - val_loss: 31.8167\n",
      "Epoch 67/200\n",
      "35/35 [==============================] - 5s 147ms/step - loss: 23.1489 - val_loss: 31.5043\n",
      "Epoch 68/200\n",
      "35/35 [==============================] - 6s 158ms/step - loss: 22.9681 - val_loss: 31.3419\n",
      "Epoch 69/200\n",
      "35/35 [==============================] - 5s 152ms/step - loss: 22.9359 - val_loss: 31.5225\n",
      "Epoch 70/200\n",
      "35/35 [==============================] - 5s 148ms/step - loss: 22.9551 - val_loss: 31.7245\n",
      "Epoch 71/200\n",
      "35/35 [==============================] - 5s 148ms/step - loss: 22.8949 - val_loss: 31.1744\n",
      "Epoch 72/200\n",
      "35/35 [==============================] - 5s 148ms/step - loss: 23.0493 - val_loss: 31.0981\n",
      "Epoch 73/200\n",
      "35/35 [==============================] - 5s 148ms/step - loss: 22.5174 - val_loss: 30.5873\n",
      "Epoch 74/200\n",
      "35/35 [==============================] - 5s 148ms/step - loss: 22.3366 - val_loss: 30.3641\n",
      "Epoch 75/200\n",
      "35/35 [==============================] - 5s 148ms/step - loss: 22.1910 - val_loss: 30.4722\n",
      "Epoch 76/200\n",
      "35/35 [==============================] - 5s 148ms/step - loss: 22.4012 - val_loss: 30.8540\n",
      "Epoch 77/200\n",
      "35/35 [==============================] - 5s 149ms/step - loss: 22.2930 - val_loss: 30.1812\n",
      "Epoch 78/200\n",
      "35/35 [==============================] - 5s 148ms/step - loss: 22.1038 - val_loss: 30.0806\n",
      "Epoch 79/200\n",
      "35/35 [==============================] - 5s 148ms/step - loss: 21.9878 - val_loss: 29.6938\n",
      "Epoch 80/200\n",
      "35/35 [==============================] - 5s 150ms/step - loss: 22.0516 - val_loss: 30.6514\n",
      "Epoch 81/200\n",
      "35/35 [==============================] - 5s 148ms/step - loss: 22.1283 - val_loss: 29.5989\n",
      "Epoch 82/200\n",
      "35/35 [==============================] - 5s 148ms/step - loss: 21.9252 - val_loss: 30.6556\n",
      "Epoch 83/200\n",
      "35/35 [==============================] - 5s 148ms/step - loss: 21.8826 - val_loss: 29.4154\n",
      "Epoch 84/200\n",
      "35/35 [==============================] - 5s 148ms/step - loss: 21.6766 - val_loss: 29.5528\n",
      "Epoch 85/200\n",
      "35/35 [==============================] - 5s 148ms/step - loss: 21.6955 - val_loss: 29.0589\n",
      "Epoch 86/200\n",
      "35/35 [==============================] - 5s 148ms/step - loss: 21.6916 - val_loss: 29.3216\n",
      "Epoch 87/200\n",
      "35/35 [==============================] - 5s 148ms/step - loss: 21.9491 - val_loss: 28.9986\n",
      "Epoch 88/200\n",
      "35/35 [==============================] - 5s 148ms/step - loss: 21.6396 - val_loss: 28.9647\n",
      "Epoch 89/200\n",
      "35/35 [==============================] - 5s 148ms/step - loss: 21.6671 - val_loss: 28.8428\n",
      "Epoch 90/200\n",
      "35/35 [==============================] - 5s 148ms/step - loss: 21.4418 - val_loss: 28.6943\n",
      "Epoch 91/200\n",
      "35/35 [==============================] - 5s 149ms/step - loss: 21.5380 - val_loss: 29.0355\n",
      "Epoch 92/200\n",
      "35/35 [==============================] - 5s 148ms/step - loss: 21.5271 - val_loss: 33.2070\n",
      "Epoch 93/200\n",
      "35/35 [==============================] - 5s 148ms/step - loss: 21.6045 - val_loss: 28.6086\n",
      "Epoch 94/200\n",
      "35/35 [==============================] - 5s 148ms/step - loss: 21.3263 - val_loss: 29.5169\n",
      "Epoch 95/200\n",
      "35/35 [==============================] - 5s 148ms/step - loss: 21.4034 - val_loss: 28.4232\n",
      "Epoch 96/200\n",
      "35/35 [==============================] - 5s 148ms/step - loss: 21.1256 - val_loss: 28.8017\n",
      "Epoch 97/200\n",
      "35/35 [==============================] - 5s 148ms/step - loss: 21.2277 - val_loss: 28.3117\n",
      "Epoch 98/200\n",
      "35/35 [==============================] - 5s 148ms/step - loss: 21.0824 - val_loss: 28.1679\n",
      "Epoch 99/200\n",
      "35/35 [==============================] - 5s 148ms/step - loss: 21.2430 - val_loss: 28.1980\n",
      "Epoch 100/200\n",
      "35/35 [==============================] - 5s 148ms/step - loss: 21.1500 - val_loss: 29.3272\n",
      "Epoch 101/200\n",
      "35/35 [==============================] - 5s 149ms/step - loss: 21.1068 - val_loss: 29.1547\n",
      "Epoch 102/200\n",
      "35/35 [==============================] - 5s 148ms/step - loss: 21.1765 - val_loss: 28.3587\n",
      "Epoch 103/200\n",
      "35/35 [==============================] - 5s 148ms/step - loss: 20.8906 - val_loss: 28.3877\n",
      "Epoch 104/200\n",
      "35/35 [==============================] - 5s 148ms/step - loss: 21.0863 - val_loss: 28.2526\n",
      "Epoch 105/200\n",
      "35/35 [==============================] - 5s 148ms/step - loss: 20.8401 - val_loss: 27.8545\n",
      "Epoch 106/200\n",
      "35/35 [==============================] - 5s 148ms/step - loss: nan - val_loss: nan\n",
      "Epoch 107/200\n",
      "35/35 [==============================] - 5s 148ms/step - loss: nan - val_loss: nan\n",
      "Epoch 108/200\n",
      "35/35 [==============================] - ETA: 0s - loss: nan"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-166-00c4e5dc55f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mroot_mean_squared_error\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             optimizer=optimizers.Adam(lr=0.001, beta_1=0.85, beta_2=0.9, epsilon=0.001, amsgrad=True))\n\u001b[0;32m----> 7\u001b[0;31m mm.fit(X_train, y_train,\n\u001b[0m\u001b[1;32m      8\u001b[0m        \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m        \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/general/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/general/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    860\u001b[0m           val_x, val_y, val_sample_weight = (\n\u001b[1;32m    861\u001b[0m               data_adapter.unpack_x_y_sample_weight(validation_data))\n\u001b[0;32m--> 862\u001b[0;31m           val_logs = self.evaluate(\n\u001b[0m\u001b[1;32m    863\u001b[0m               \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m               \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/general/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/general/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[1;32m   1079\u001b[0m                 step_num=step):\n\u001b[1;32m   1080\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1081\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1082\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/general/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/general/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    616\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m~/venvs/general/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/general/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1659\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \"\"\"\n\u001b[0;32m-> 1661\u001b[0;31m     return self._call_flat(\n\u001b[0m\u001b[1;32m   1662\u001b[0m         (t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[0;32m~/venvs/general/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1745\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/venvs/general/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    591\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    594\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/general/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1)) \n",
    "mm.summary()\n",
    "mm.compile(\n",
    "            loss=root_mean_squared_error,\n",
    "            optimizer=optimizers.Adam(lr=0.001, beta_1=0.85, beta_2=0.9, epsilon=0.001, amsgrad=True))\n",
    "mm.fit(X_train, X_train,\n",
    "       validation_data=(X_test, y_test),\n",
    "       batch_size=512,\n",
    "       shuffle=True,\n",
    "       epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.fit(train_dataset, test_dataset, epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "v = model.predict(train_dataset, step=5)\n",
    "#predicted_probas = v\n",
    "#idx_tensor = transformer.transformerY['scale'].value_ranges[:-1]\n",
    "#pred_cont = np.sum(predicted_probas * idx_tensor, axis=1)\n",
    "pred_cont = v\n",
    "true_data = get_features(train_dataset, step=5, window=30, features=['RUL'])\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 5))\n",
    "ax.plot(true_data['RUL'][0:530], label='True')\n",
    "ax.plot(pred_cont[0:500], label='Predicted')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "v = model.predict(test_dataset, step=1)\n",
    "#predicted_probas = v\n",
    "#idx_tensor = transformer.transformerY['scale'].value_ranges[:-1]\n",
    "#pred_cont = np.sum(predicted_probas * idx_tensor, axis=1)\n",
    "pred_cont = v\n",
    "true_data = get_features(test_dataset, step=1, window=30, features=['RUL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(15, 5))\n",
    "ax.plot(true_data['RUL'][0:1000], label='True')\n",
    "ax.plot(pred_cont[0:1000], label='Predicted')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.concatenate(model.true_values(test_dataset, step=5)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.true_values(test_dataset, step=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset[0]['RUL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
